{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_availability = {}\n",
    "with open(\"../pkl/available_kb_datasets.pkl\",\"rb\") as pkl: dataset_availability[\"kb_available\"] = pickle.load(pkl)\n",
    "with open(\"../pkl/unavailable_kb_datasets.pkl\",\"rb\") as pkl: dataset_availability[\"kb_unavailable\"] = pickle.load(pkl)\n",
    "with open(\"../pkl/available_qa_datasets.pkl\",\"rb\") as pkl: dataset_availability[\"qa_available\"] = pickle.load(pkl)\n",
    "with open(\"../pkl/unavailable_qa_datasets.pkl\",\"rb\") as pkl: dataset_availability[\"qa_unavailable\"] = pickle.load(pkl)\n",
    "with open(\"../pkl/available_t2t_datasets.pkl\",\"rb\") as pkl: dataset_availability[\"t2t_available\"] = pickle.load(pkl)\n",
    "with open(\"../pkl/unavailable_t2t_datasets.pkl\",\"rb\") as pkl: dataset_availability[\"t2t_unavailable\"] = pickle.load(pkl)\n",
    "with open(\"../pkl/available_text_datasets.pkl\",\"rb\") as pkl: dataset_availability[\"text_available\"] = pickle.load(pkl)\n",
    "with open(\"../pkl/unavailable_text_datasets.pkl\",\"rb\") as pkl: dataset_availability[\"text_unavailable\"] = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(df):    \n",
    "    try:    \n",
    "        entities = df[df[\"entities\"].apply(len) != 0][\"entities\"].tolist()\n",
    "        entities = [item for sublist in entities for item in sublist]\n",
    "        new_entities = {}\n",
    "        for entity in entities: new_entities[entity[\"id\"]] = f\"{entity['type']}\"\n",
    "        for entity in set([new_entities[entity] for entity in new_entities.keys()]): print(entity)\n",
    "        print()\n",
    "        return new_entities\n",
    "    except: return []\n",
    "    \n",
    "def get_relations(df,entities):\n",
    "    try:\n",
    "        relations = df[df[\"relations\"].apply(len) != 0][\"relations\"].tolist()\n",
    "        relations = [item for sublist in relations for item in sublist]\n",
    "        for relation in set([f\"{entities[relation['arg1_id']]} - {relation['type'].upper()} - {entities[relation['arg2_id']]}\" for relation in relations]):\n",
    "            print(relation)\n",
    "    except: pass\n",
    "\n",
    "def select_largest_split(dataset_dict):\n",
    "    max_split, max_rows, max_dataset = None, 0, None\n",
    "    for split, dataset in dataset_dict.items():\n",
    "        if dataset.num_rows > max_rows:\n",
    "            max_split, max_rows, max_dataset = split, dataset.num_rows, dataset\n",
    "    return max_split, max_rows, max_dataset\n",
    "\n",
    "def get_qa(df):\n",
    "    question_types = df[\"type\"].unique().tolist()\n",
    "    samples = list(zip(df['question'].head(5), df['answer'].head(5)))\n",
    "    print(sorted(question_types))\n",
    "    print(\"\\nSAMPLES:\\n\")\n",
    "    for sample in samples:\n",
    "        print(f\"QUESTION: {sample[0]}\")\n",
    "        print(f\"ANSWER: {sample[1]}\")\n",
    "        print()\n",
    "\n",
    "def get_text_class(df):\n",
    "    labels_list = df['labels'].tolist()\n",
    "    samples = list(zip(df['text'].head(5), df['labels'].head(5)))\n",
    "    labels = []\n",
    "    for label in labels_list:\n",
    "        labels.extend(label)\n",
    "    labels = set(labels)\n",
    "    print(labels,\"\\n\\nSAMPLES:\\n\")\n",
    "    for sample in samples:\n",
    "        print(\"TEXT: \",sample[0].replace(\"\\n\",\" \"))\n",
    "        print(f\"LABELS: {sample[1]}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def get_t2t(df):\n",
    "    samples = list(zip(df['text_1'].head(5), df['text_2'].head(5)))\n",
    "    for (sample1,sample2) in samples:\n",
    "        sample1 = sample1.replace('\\n', ' ').strip()\n",
    "        sample2 = sample2.replace('\\n', ' ').strip()\n",
    "        print(f\"TEXT1: {sample1}\")\n",
    "        print(f\"TEXT2: {sample2}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in dataset_availability[\"kb_available\"]:\n",
    "    dataset = load_dataset(f\"bigbio/{elem[0]}\",name=elem[1],trust_remote_code=True)\n",
    "    split, rows, df = select_largest_split(dataset)\n",
    "    df = df.to_pandas()\n",
    "    print(f\"DATASET\\n{elem} - {split} - {rows}\\n\\nENTITIES:\")\n",
    "    entities = get_entities(df)\n",
    "    print(f\"RELATIONS:\")\n",
    "    get_relations(df,entities)\n",
    "    print(\"\\n--------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in dataset_availability[\"qa_available\"]:\n",
    "    dataset = load_dataset(f\"bigbio/{elem[0]}\",name=elem[1],trust_remote_code=True)\n",
    "    split, rows, df = select_largest_split(dataset)\n",
    "    df = df.to_pandas()\n",
    "    print(f\"DATASET\\n{elem} - {split} - {rows}\\n\\nQUESTION TYPES:\")\n",
    "    get_qa(df)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in dataset_availability[\"text_available\"]:\n",
    "    dataset = load_dataset(f\"bigbio/{elem[0]}\",name=elem[1],trust_remote_code=True)\n",
    "    split, rows, df = select_largest_split(dataset)\n",
    "    df = df.to_pandas()\n",
    "    print(f\"DATASET\\n{elem} - {split} - {rows}\\n\\nTEXT LABELS:\")\n",
    "    get_text_class(df)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in dataset_availability[\"t2t_available\"]:\n",
    "    dataset = load_dataset(f\"bigbio/{elem[0]}\",name=elem[1],trust_remote_code=True)\n",
    "    split, rows, df = select_largest_split(dataset)\n",
    "    df = df.to_pandas()\n",
    "    print(f\"DATASET\\n{elem} - {split} - {rows}\\n\\nT2T SAMPLES:\\n\")\n",
    "    get_t2t(df)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigbio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
